{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1Final Report",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbparsons/code/blob/master/Network%20Analysis-%20Lady%20Gaga%2C%20Katy%20Perry%2C%20and%20Taylor%20Swift\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHH69y2RdbJu",
        "colab_type": "text"
      },
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5T9f1vM2Ax_",
        "colab_type": "code",
        "outputId": "05d780e4-27aa-4aca-a2d7-381cd99b448c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rfr0IBP0HYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMagLu4Kkrad",
        "colab_type": "code",
        "outputId": "17981afc-6593-48f5-b3c2-a2b5c6354352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "try:\n",
        "    import birdy\n",
        "except ModuleNotFoundError:\n",
        "    !pip install birdy\n",
        "try:\n",
        "    import ratelimiter\n",
        "except ModuleNotFoundError:\n",
        "    !pip install ratelimiter\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting birdy\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/30/3f825b8d4248ebd9de9d218ba4b931c93be664e077c328c4b6dd19eb9d8a/birdy-0.3.2.tar.gz\n",
            "Requirement already satisfied: requests>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from birdy) (2.21.0)\n",
            "Requirement already satisfied: requests_oauthlib>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from birdy) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.3->birdy) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests_oauthlib>=0.3.2->birdy) (3.1.0)\n",
            "Building wheels for collected packages: birdy\n",
            "  Building wheel for birdy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for birdy: filename=birdy-0.3.2-cp36-none-any.whl size=10853 sha256=2c5cc58481104d0cb628364aa8b72fdd84e768bc21a1aa2e164c52508caac4b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/f9/a7/928ef99a65cfa8182e42fb0a052b0a61faa69b7d085fae2723\n",
            "Successfully built birdy\n",
            "Installing collected packages: birdy\n",
            "Successfully installed birdy-0.3.2\n",
            "Collecting ratelimiter\n",
            "  Downloading https://files.pythonhosted.org/packages/51/80/2164fa1e863ad52cc8d870855fba0fbb51edd943edffd516d54b5f6f8ff8/ratelimiter-1.2.0.post0-py3-none-any.whl\n",
            "Installing collected packages: ratelimiter\n",
            "Successfully installed ratelimiter-1.2.0.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s5CGwEkBgXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json, os, sys, time\n",
        "from zipfile import ZipFile\n",
        "from birdy.twitter import AppClient, UserClient, TwitterRateLimitError\n",
        "from ratelimiter import RateLimiter\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Credentials can be found by selecting the \"Keys and tokens\" tab for your\n",
        "application selected from:\n",
        "\n",
        "https://developer.twitter.com/en/apps/\n",
        "\"\"\"\n",
        "DEFAULT_MAX_TWEETS = 10_000\n",
        "\n",
        "def limited(until):\n",
        "    duration = int(round(until - time.time()))\n",
        "    print('Rate limited, sleeping for {:d} seconds'.format(duration))\n",
        "\n",
        "\n",
        "class TwitterSearcher():\n",
        "\n",
        "    def __init__(self, consumer_key, consumer_secret,\n",
        "                 output_dir, max_tweets=DEFAULT_MAX_TWEETS):\n",
        "        self._consumer_key = consumer_key\n",
        "        self._consumer_secret = consumer_secret\n",
        "        self._output_dir = output_dir\n",
        "        self._max_tweets = max_tweets\n",
        "        self._client = None\n",
        "        self._max_id = None\n",
        "\n",
        "    def client(self):\n",
        "        if self._client is None:\n",
        "            _cl = AppClient(self._consumer_key, self._consumer_secret)\n",
        "            access_token = _cl.get_access_token()\n",
        "            self._client = AppClient(\n",
        "                self._consumer_key, self._consumer_secret, access_token)\n",
        "        return self._client\n",
        "\n",
        "    @RateLimiter(max_calls=440, period=60*15, callback=limited)\n",
        "    def fetch_tweets(self, query):\n",
        "        print(f'Fetching: \"{query}\" TO MAX ID: {self._max_id}')\n",
        "        client = self.client()\n",
        "        try:\n",
        "            tweets = client.api.search.tweets.get(\n",
        "                q=query,\n",
        "                count=100,\n",
        "                max_id=self._max_id).data['statuses']\n",
        "        except TwitterRateLimitError:\n",
        "            print(\"You've reached your Twitter API rate limit. \"\\\n",
        "                \"Wait 15 minutes before trying again\")\n",
        "            sys.exit()\n",
        "        try:\n",
        "            id_ = min([tweet['id'] for tweet in tweets])\n",
        "        except ValueError:\n",
        "            return None\n",
        "        if self._max_id is None or id_ <= self._max_id:\n",
        "            self._max_id = id_ - 1\n",
        "        return tweets\n",
        "\n",
        "    def initialize_max_id(self, file_list):\n",
        "        for fn in file_list:\n",
        "            n = int(fn.split('.')[0])\n",
        "            if self._max_id is None or n < self._max_id:\n",
        "                self._max_id = n - 1\n",
        "        if self._max_id is not None:\n",
        "            print('Found previously fetched tweets. '\\\n",
        "                  'Setting max_id to %d' % self._max_id)\n",
        "\n",
        "    def halt(self, _id):\n",
        "        print('Reached historically fetched ID: %d' % _id)\n",
        "        print('In order to re-fetch older tweets, ' \\\n",
        "            'remove tweets from the output directory or output zip file.')\n",
        "        print('\\n!!IMPORTANT: Tweets older than 7 days will not be re-fetched')\n",
        "        return\n",
        "\n",
        "    def search(self, query, dozip=True, verbose=False):\n",
        "        output_dir = os.path.join(self._output_dir, '_'.join(query.split()))\n",
        "        outzip = None\n",
        "        self._max_id = None\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        if dozip:\n",
        "            fn = os.path.join(output_dir, '%s.zip' % '_'.join(query.split()))\n",
        "            outzip = ZipFile(fn, 'a')\n",
        "        if dozip:\n",
        "            file_list = [f for f in outzip.namelist() if f.endswith('.json')]\n",
        "        else:\n",
        "            file_list = [f for f in os.listdir(output_dir) if f.endswith('.json')]\n",
        "        self.initialize_max_id(file_list)\n",
        "        try:\n",
        "            while True:\n",
        "                tweets = self.fetch_tweets(query)\n",
        "                if tweets is None:\n",
        "                    print('Search Completed')\n",
        "                    return\n",
        "                for tweet in tweets:\n",
        "                    if verbose:\n",
        "                        print(tweet['id'])\n",
        "                    fn = '%d.json' % tweet['id']\n",
        "                    if dozip:\n",
        "                        if fn in (file_list):\n",
        "                            self.halt(tweet['id'])\n",
        "                        else:\n",
        "                            outzip.writestr(fn, json.dumps(tweet, indent=4))\n",
        "                            file_list.append(fn)\n",
        "                    else:\n",
        "                        path = os.path.join(output_dir, fn)\n",
        "                        if fn in (file_list):\n",
        "                            self.halt(tweet['id'])\n",
        "                        else:\n",
        "                            with open(path, 'w') as outfile:\n",
        "                                json.dump(tweet, outfile, indent=4)\n",
        "                            file_list.append(fn)\n",
        "                    if len(file_list) >= self._max_tweets:\n",
        "                        print('Reached maximum tweet limit of: %d' % self._max_tweets)\n",
        "                        return\n",
        "        except KeyboardInterrupt:\n",
        "            print('Search interrupted. Re-run to continue.')\n",
        "            sys.exit()\n",
        "        except:\n",
        "            raise\n",
        "        finally:\n",
        "            if outzip is not None:\n",
        "                outzip.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-JXs_MEBn3j",
        "colab_type": "code",
        "outputId": "6671c38c-0b31-4a08-ec11-ba36c370c180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "CONSUMER_KEY = 'rEiui1v4bXuBsWtkEzumsLG6A'\n",
        "CONSUMER_SECRET = 'kRPjyBYpwzdZqEi1ktpqwNtEaWemMsEN04WUvD9fSReVTZ1mJA'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/Network Analysis'\n",
        "\n",
        "searcher = TwitterSearcher(CONSUMER_KEY, CONSUMER_SECRET, OUTPUT_DIR)\n",
        "searcher.search('@katyperry', dozip=True)\n",
        "searcher.search('@ladygaga', dozip=True)\n",
        "searcher.search('@taylorswift13', dozip=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching: \"@taylorswift13\" TO MAX ID: None\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231445431466504191\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231442740820561920\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231440102599143423\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231437707307470847\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231435876858724352\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231433039676203010\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231430500595752959\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231427112470376448\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231424566137847807\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231422148859908096\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231420610581975040\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231418433864175615\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231416060517273599\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231413648553234431\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231411717508341761\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231409886887632895\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231408415647334399\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231406674252042241\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231405016210116619\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231403859999895551\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231403099627102214\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231400075013701632\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231398979964825599\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231397414654771199\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231396590750814208\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231394913264095232\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231392606556282879\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231389793302536192\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231388376953229311\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231387708842553348\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231386105477521408\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231384474606329857\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231381284771041280\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231379130274516998\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231376681031606271\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231372927351676928\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231371204562477055\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231368217198780415\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231366909355970560\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231365485272260607\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231364235101233152\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231363117554294783\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231361681990836225\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231360395690070015\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231358796104175616\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231356014399717375\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231354926816792577\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231353464606748671\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231352079777595391\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231350910732787714\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231350088112447488\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231349298970927104\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231348185714642949\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231345761910239231\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231344337314729984\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231343642104582143\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231341923350335489\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231340376654516223\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231339714088767489\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231339239985569791\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231337969136685056\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231336648631291903\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231335744372957184\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231334166161166337\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231333013449052161\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231332166065254400\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231331302663761920\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231330028702588928\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231328720859926528\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231327509599445001\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231326382463938559\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231325988673421312\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231325662230740992\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231324912041811967\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231324249488531455\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231321460674846724\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231320404381356033\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231316743114674181\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231313615359266820\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231311000537849856\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231308159077277697\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231306476507017217\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231305315653685248\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231304500264259584\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231303022518554624\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231302318005669888\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231301473155403775\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231300610705760259\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231300013621436417\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231299707298865159\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231299033236484095\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231298349384466433\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231297621215633408\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231297020780011520\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231296262525186048\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231295856336306176\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231295601205301248\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231295024438169601\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231294297925832703\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231292365853331455\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231290474897407999\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231288779048181761\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231287284248580096\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231285790891749381\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231284729980346367\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231283838183018495\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231280821627023359\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231279554225897472\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231279165200044031\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231277246960275455\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231274117510418431\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231272274575187967\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231270337641316358\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231268962135990272\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231267882962690049\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231266112387604479\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231264773150400511\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231262664992722945\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231260719687065602\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231258303222730754\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231255213018877951\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231252325370560512\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231250248200118273\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231248198255009793\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231245975815061505\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231244705448353793\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231243241678196735\n",
            "Fetching: \"@taylorswift13\" TO MAX ID: 1231241435678203903\n",
            "Reached maximum tweet limit of: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U5cR8U51F_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetzipfiles = glob.glob('/content/drive/My Drive/Colab Notebooks/Network Analysis/*.zip') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxI3FMn31eef",
        "colab_type": "code",
        "outputId": "9c7beda8-7728-4d1f-8636-c006d85624e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "tweetzipfiles"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Colab Notebooks/Network Analysis/@katyperry.zip',\n",
              " '/content/drive/My Drive/Colab Notebooks/Network Analysis/@ladygaga.zip',\n",
              " '/content/drive/My Drive/Colab Notebooks/Network Analysis/@taylorswift13.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDDelwDfmq8A",
        "colab_type": "text"
      },
      "source": [
        "#Creating Priliminary Edgelist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdkvOuCj3hND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniqueusers = {}\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in uniqueusers:\n",
        "      uniqueusers[userwhotweeted] += 1\n",
        "    if userwhotweeted not in uniqueusers:\n",
        "      uniqueusers[userwhotweeted] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RyPGjC9XMV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userstoinclude = set()\n",
        "usercount = 0\n",
        "for auser in uniqueusers:\n",
        "  if uniqueusers[auser] > 2:\n",
        "    usercount += 1\n",
        "    userstoinclude.add(auser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI5VAbUtYLTU",
        "colab_type": "code",
        "outputId": "2a946e49-4a06-4801-c5c1-a10880a91504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/artistsedgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target']\n",
        "csvwriter.writerow(header)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKUe9yJTYLXD",
        "colab_type": "code",
        "outputId": "3d9b0037-99df-4552-d85c-f512da465699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Writing edge list')\n",
        "count = 0\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  count += 1\n",
        "  if count % 1000 == 0:\n",
        "    print(count)\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in userstoinclude:\n",
        "      users = tweetjson['entities']['user_mentions']\n",
        "      if len(users) > 0:\n",
        "        for auser in users:\n",
        "          screenname = auser['screen_name']\n",
        "          row = [userwhotweeted, screenname]\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing edge list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeAbw5U1tiVW",
        "colab_type": "text"
      },
      "source": [
        "# Removing Bots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RkxSOfcuXxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniqueusers = {}\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in uniqueusers:\n",
        "      uniqueusers[userwhotweeted] += 1\n",
        "    if userwhotweeted not in uniqueusers:\n",
        "      uniqueusers[userwhotweeted] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NABVp59nubAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userstoinclude = []\n",
        "usercount = 0\n",
        "for auser in uniqueusers:\n",
        "  if uniqueusers[auser] > 2:\n",
        "    usercount += 1\n",
        "    userstoinclude.append(auser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM7eeki7tmRM",
        "colab_type": "code",
        "outputId": "f3c9386b-ae25-4140-bc26-01a05c20e4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "pip install botometer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting botometer\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/18/49b1acb456765ee18d3afc3cac718225c196c06bea18e752d478153edcc5/botometer-1.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from botometer) (2.21.0)\n",
            "Requirement already satisfied: tweepy>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from botometer) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->botometer) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.3.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.5.0->botometer) (1.12.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.5.0->botometer) (3.1.0)\n",
            "Building wheels for collected packages: botometer\n",
            "  Building wheel for botometer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botometer: filename=botometer-1.4-cp36-none-any.whl size=2874 sha256=81065443a05db4fe1bf8b3d0e1c9f1805a95b3cfd34025747f6c88738768ac47\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/65/0b/ce361d6acc341ed4479c4346cd038f8bc462d92287745f5231\n",
            "Successfully built botometer\n",
            "Installing collected packages: botometer\n",
            "Successfully installed botometer-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUw2QSXStxCl",
        "colab_type": "code",
        "outputId": "a0136709-3e90-47e2-cc82-abe12a210313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "pip install requests tweepy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.7.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFCFjD_8tzOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import tweepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdjtaWRZtzRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import botometer\n",
        "\n",
        "rapidapi_key = \"70924aa0e1msha726265a264ea8cp1fcb13jsn944cbd144368\" # now it's called rapidapi key\n",
        "twitter_app_auth = {\n",
        "    'consumer_key' : 'rEiui1v4bXuBsWtkEzumsLG6A',\n",
        "    'consumer_secret' : 'kRPjyBYpwzdZqEi1ktpqwNtEaWemMsEN04WUvD9fSReVTZ1mJA',\n",
        "    'access_token': '374353460-a4q6fJX10wE7QE91DeHRjHJKLVoJkPgkbqcRQpS2',\n",
        "    'access_token_secret': 'viVp2nISoMvwFs9hjMBpdGcX0duT8vkDbtMzFVXg3QuPg',\n",
        "  }\n",
        "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
        "                          rapidapi_key=rapidapi_key,\n",
        "                          **twitter_app_auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPKScthHtzT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accounts = userstoinclude\n",
        "botscore = {} \n",
        "count = 0\n",
        "for screen_name, result in bom.check_accounts_in(accounts):\n",
        "  count += 1\n",
        "  if count % 10 == 0:\n",
        "    print(count)\n",
        "  if 'scores' in result.keys():\n",
        "    botscore[screen_name] = result['scores']['english']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmIX3V-RDHOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "botlist = []\n",
        "for i in botscore:\n",
        "  if botscore[i] > 0.43:\n",
        "      botlist.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0MmciesFnL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "botlist1 = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/botscore.csv', 'w') \n",
        "writer = csv.writer(botlist1)\n",
        "\n",
        "print('Writing Bot List')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_c2heGhFrxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "for i in botscore:\n",
        "  count += 1\n",
        "  if count % 10 == 0:\n",
        "    print(count)\n",
        "  writer.writerow(botlist)\n",
        "botlist1.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEwHFRpPFuAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "botdata = {}\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Network Analysis/botscore.csv', 'r') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "          botdata = row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGRcVJnKu2gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniqueusers = {}\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if userwhotweeted in uniqueusers:\n",
        "        uniqueusers[userwhotweeted] += 1\n",
        "      if userwhotweeted not in uniqueusers:\n",
        "        uniqueusers[userwhotweeted] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWE0TpyppMqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userstoinclude = set()\n",
        "usercount = 0\n",
        "for auser in uniqueusers:\n",
        "  if uniqueusers[auser] > 2:\n",
        "    usercount += 1\n",
        "    userstoinclude.add(auser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SV0Msv-pNqe",
        "colab_type": "code",
        "outputId": "3dd0aac3-5d03-4ee0-9d1c-8ef0c73d7585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/alluserstedgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target']\n",
        "csvwriter.writerow(header)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoXvw6oepTmT",
        "colab_type": "code",
        "outputId": "62cab3f0-4fbc-4cc3-9e4a-2c7570d2fcf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Writing edge list')\n",
        "count = 0\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  count += 1\n",
        "  if count % 1000 == 0:\n",
        "    print(count)\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in userstoinclude:\n",
        "      users = tweetjson['entities']['user_mentions']\n",
        "      if len(users) > 0:\n",
        "        for auser in users:\n",
        "          screenname = auser['screen_name']\n",
        "          row = [userwhotweeted, screenname]\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing edge list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtayY-BbrNZD",
        "colab_type": "text"
      },
      "source": [
        "# Segmenting by Amounts of Followers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l9ighmC2ZGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "highcount = {}\n",
        "lowcount = {}\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if tweetjson['user'][\"followers_count\"] >= 10000:\n",
        "        if userwhotweeted in highcount:\n",
        "          highcount[userwhotweeted] += 1\n",
        "        if userwhotweeted not in highcount:\n",
        "          highcount[userwhotweeted] = 1\n",
        "      if tweetjson['user'][\"followers_count\"] < 10000:\n",
        "        if userwhotweeted in lowcount:\n",
        "         lowcount[userwhotweeted] += 1\n",
        "        if userwhotweeted not in lowcount:\n",
        "          lowcount[userwhotweeted] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShXHfAoOy_q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userstoinclude = set()\n",
        "usercount = 0\n",
        "for auser in highcount:\n",
        "  if highcount[auser] > 2:\n",
        "    usercount += 1\n",
        "    userstoinclude.add(auser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1RpFeyiy_yf",
        "colab_type": "code",
        "outputId": "a18fe29e-2e52-48d2-9ec7-725c98efdfed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/highcountedgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target']\n",
        "csvwriter.writerow(header)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cryIK0sMzaLr",
        "colab_type": "code",
        "outputId": "e21884d1-704c-47ee-9496-e92bb46281b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Writing edge list')\n",
        "count = 0\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  count += 1\n",
        "  if count % 1000 == 0:\n",
        "    print(count)\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in userstoinclude:\n",
        "      users = tweetjson['entities']['user_mentions']\n",
        "      if len(users) > 0:\n",
        "        for auser in users:\n",
        "          screenname = auser['screen_name']\n",
        "          row = [userwhotweeted, screenname]\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing edge list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jaovd50z1T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userstoinclude = set()\n",
        "usercount = 0\n",
        "for auser in lowcount:\n",
        "  if lowcount[auser] > 2:\n",
        "    usercount += 1\n",
        "    userstoinclude.add(auser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xMhdCHpz1aj",
        "colab_type": "code",
        "outputId": "c89b7adc-df8f-40fd-eec7-a54cfdefcf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/lowcountedgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target']\n",
        "csvwriter.writerow(header)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CZFAv-oz1e2",
        "colab_type": "code",
        "outputId": "9d3e5b65-958b-4446-b7b1-e5d7adb2e6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Writing edge list')\n",
        "count = 0\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  count += 1\n",
        "  if count % 1000 == 0:\n",
        "    print(count)\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted in userstoinclude:\n",
        "      users = tweetjson['entities']['user_mentions']\n",
        "      if len(users) > 0:\n",
        "        for auser in users:\n",
        "          screenname = auser['screen_name']\n",
        "          row = [userwhotweeted, screenname]\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing edge list\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1IdmPPIqjfV",
        "colab_type": "text"
      },
      "source": [
        "#Creating Priliminary Semantic Edgelist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmEWn8WwJFxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import csv\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import itertools\n",
        "import zipfile\n",
        "import json\n",
        "punctuation = string.punctuation\n",
        "stopwordsset = set(stopwords.words(\"english\"))\n",
        "stopwordsset.add('rt')\n",
        "stopwordsset.add(\"'s\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC9-mLrKJIL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing urls\n",
        "def removeURL(text):\n",
        "  result = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return result\n",
        "#Extracting contextual words from a sentence\n",
        "# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n",
        "def tokenize(text):\n",
        "  #lower case\n",
        "  text = text.lower()\n",
        "  #split into individual words\n",
        "  words = word_tokenize(text)\n",
        "  return words\n",
        "#stem - peaches : peach : reduce the number of repeated words\n",
        "def stem(tokenizedtext):\n",
        "  rootwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = ps.stem(aword)\n",
        "    rootwords.append(aword)\n",
        "  return rootwords\n",
        "#removes useless words such as a, an, the\n",
        "def stopWords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in stopwordsset:\n",
        "      goodwords.append(aword)\n",
        "  return goodwords\n",
        "# feature reduction. taking words and getting their roots and graphing only the root words\n",
        "def lemmatizer(tokenizedtext):\n",
        "  lemmawords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = wn.lemmatize(aword)\n",
        "    lemmawords.append(aword)\n",
        "  return lemmawords\n",
        "#inputs a list of tokens and returns a list of unpunctuated tokens/words\n",
        "def removePunctuation(tokenizedtext):\n",
        "  nopunctwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in punctuation:\n",
        "      nopunctwords.append(aword)\n",
        "  cleanedwords = []\n",
        "  for aword in nopunctwords:\n",
        "    aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
        "    cleanedwords.append(aword)\n",
        "  return cleanedwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFIDjkPMJRlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    #natural language preprocessing- cleaning the tweets\n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "    #sleep(10)\n",
        "\n",
        "    #print(tokenizedtext)\n",
        "    #print(nostopwordstext)\n",
        "    #print(lemmatizedtext)\n",
        "    #print(nopuncttext)\n",
        "\n",
        "    for aword in nopuncttext:\n",
        "      if aword in uniquewords:\n",
        "        uniquewords[aword] += 1\n",
        "      if aword not in uniquewords:\n",
        "        uniquewords[aword] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuzjT4uIJUhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of unique words: key - word; value = number of times the word appears\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "  if uniquewords[aword] > 25: #change number to get around 1000 words\n",
        "    wordcount += 1\n",
        "    wordstoinclude.add(aword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdR4pYbGJ0dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/artists.semantic.edgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target', 'Type'] # add Type to mention this is undirected \n",
        "csvwriter.writerow(header)\n",
        "\n",
        "print('Writing Edge List')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrth22iTJ0ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    #natural language preprocessing- cleaning the tweets\n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "\n",
        "    goodwords = []\n",
        "    for aword in nopuncttext:\n",
        "      if aword in wordstoinclude:\n",
        "        goodwords.append(aword.replace(',',''))\n",
        "    \n",
        "    allcombos = itertools.combinations(goodwords, 2) # set this to number of words\n",
        "    for acombo in allcombos:\n",
        "      row = []\n",
        "      for anode in acombo:\n",
        "        row.append(anode)\n",
        "      row.append('Undirected')\n",
        "      csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bcxs3yvq0yJ",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing/Sentiment Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACuhOh6Sp9YP",
        "colab_type": "code",
        "outputId": "df177852-2ccc-4c37-dda6-405ad01c5985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import csv\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import itertools\n",
        "import zipfile\n",
        "import json\n",
        "punctuation = string.punctuation\n",
        "stopwordsset = set(stopwords.words(\"english\"))\n",
        "stopwordsset.add('rt')\n",
        "stopwordsset.add(\"'s\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtkymDwmqALW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing urls\n",
        "def removeURL(text):\n",
        "  result = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return result\n",
        "#Extracting contextual words from a sentence\n",
        "# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n",
        "def tokenize(text):\n",
        "  #lower case\n",
        "  text = text.lower()\n",
        "  #split into individual words\n",
        "  words = word_tokenize(text)\n",
        "  return words\n",
        "#stem - peaches : peach : reduce the number of repeated words\n",
        "def stem(tokenizedtext):\n",
        "  rootwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = ps.stem(aword)\n",
        "    rootwords.append(aword)\n",
        "  return rootwords\n",
        "#removes useless words such as a, an, the\n",
        "def stopWords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in stopwordsset:\n",
        "      goodwords.append(aword)\n",
        "  return goodwords\n",
        "# feature reduction. taking words and getting their roots and graphing only the root words\n",
        "def lemmatizer(tokenizedtext):\n",
        "  lemmawords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = wn.lemmatize(aword)\n",
        "    lemmawords.append(aword)\n",
        "  return lemmawords\n",
        "#inputs a list of tokens and returns a list of unpunctuated tokens/words\n",
        "def removePunctuation(tokenizedtext):\n",
        "  nopunctwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in punctuation:\n",
        "      nopunctwords.append(aword)\n",
        "  cleanedwords = []\n",
        "  for aword in nopunctwords:\n",
        "    aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
        "    cleanedwords.append(aword)\n",
        "  return cleanedwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pxfvUIBqJNz",
        "colab_type": "code",
        "outputId": "d5342ad8-fdc7-43c4-dd5f-fffab08a56dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "pip install -U textblob"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foz30trUqOQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81KFcpYdMwtA",
        "colab_type": "code",
        "outputId": "d7bde09f-8d36-4fe5-8164-21fd3b354a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if sentimentscore < 0:\n",
        "        #natural language preprocessing- cleaning the tweets\n",
        "        nourlstext = removeURL(text)\n",
        "        tokenizedtext = tokenize(nourlstext)\n",
        "        nostopwordstext = stopWords(tokenizedtext)\n",
        "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "        nopuncttext = removePunctuation(lemmatizedtext)\n",
        "        #sleep(10)\n",
        "\n",
        "        #print(tokenizedtext)\n",
        "        #print(nostopwordstext)\n",
        "        #print(lemmatizedtext)\n",
        "        #print(nopuncttext)\n",
        "\n",
        "        for aword in nopuncttext:\n",
        "          if aword in uniquewords:\n",
        "            uniquewords[aword] += 1\n",
        "          if aword not in uniquewords:\n",
        "            uniquewords[aword] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R74NGyr1qVS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of unique words: key - word; value = number of times the word appears\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "  if uniquewords[aword] > 25: #change number to get around 1000 words\n",
        "    wordcount += 1\n",
        "    wordstoinclude.add(aword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4BW0lbtqYS5",
        "colab_type": "code",
        "outputId": "c150c447-a579-4326-cbb7-4a1bff0106b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/negative.sentiment.artists.semantic.edgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target', 'Type'] # add Type to mention this is undirected \n",
        "csvwriter.writerow(header)\n",
        "\n",
        "print('Writing Edge List')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Edge List\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZQGR_8qRIE",
        "colab_type": "code",
        "outputId": "4289b176-6e32-4cf6-8af1-bd048e6abd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if sentimentscore < 0:\n",
        "        #natural language preprocessing- cleaning the tweets\n",
        "        nourlstext = removeURL(text)\n",
        "        tokenizedtext = tokenize(nourlstext)\n",
        "        nostopwordstext = stopWords(tokenizedtext)\n",
        "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "        nopuncttext = removePunctuation(lemmatizedtext)\n",
        "        \n",
        "\n",
        "        goodwords = []\n",
        "        for aword in nopuncttext:\n",
        "          if aword in wordstoinclude:\n",
        "            goodwords.append(aword.replace(',',''))\n",
        "        \n",
        "        allcombos = itertools.combinations(goodwords, 2) # set this to number of words\n",
        "        for acombo in allcombos:\n",
        "          row = []\n",
        "          for anode in acombo:\n",
        "            row.append(anode)\n",
        "          row.append('Undirected')\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86mVgZkOZuMo",
        "colab_type": "code",
        "outputId": "a35780de-b219-4132-d91d-e487d97d584f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if sentimentscore > 0:\n",
        "        #natural language preprocessing- cleaning the tweets\n",
        "        nourlstext = removeURL(text)\n",
        "        tokenizedtext = tokenize(nourlstext)\n",
        "        nostopwordstext = stopWords(tokenizedtext)\n",
        "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "        nopuncttext = removePunctuation(lemmatizedtext)\n",
        "        #sleep(10)\n",
        "\n",
        "        #print(tokenizedtext)\n",
        "        #print(nostopwordstext)\n",
        "        #print(lemmatizedtext)\n",
        "        #print(nopuncttext)\n",
        "\n",
        "        for aword in nopuncttext:\n",
        "          if aword in uniquewords:\n",
        "            uniquewords[aword] += 1\n",
        "          if aword not in uniquewords:\n",
        "            uniquewords[aword] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNzGfSMyZ3M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of unique words: key - word; value = number of times the word appears\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "  if uniquewords[aword] > 25: #change number to get around 1000 words\n",
        "    wordcount += 1\n",
        "    wordstoinclude.add(aword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7cBwpTZ3Sr",
        "colab_type": "code",
        "outputId": "c3c91172-18b7-4204-e7b5-b3f00bd84f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/positive.sentiment.artists.semantic.edgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target', 'Type'] # add Type to mention this is undirected \n",
        "csvwriter.writerow(header)\n",
        "\n",
        "print('Writing Edge List')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Edge List\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT3CrQ8qZ8ur",
        "colab_type": "code",
        "outputId": "e6958339-ad73-498e-849d-c78b1ffc256d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      if sentimentscore > 0:\n",
        "        #natural language preprocessing- cleaning the tweets\n",
        "        nourlstext = removeURL(text)\n",
        "        tokenizedtext = tokenize(nourlstext)\n",
        "        nostopwordstext = stopWords(tokenizedtext)\n",
        "        lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "        nopuncttext = removePunctuation(lemmatizedtext)\n",
        "        \n",
        "\n",
        "        goodwords = []\n",
        "        for aword in nopuncttext:\n",
        "          if aword in wordstoinclude:\n",
        "            goodwords.append(aword.replace(',',''))\n",
        "        \n",
        "        allcombos = itertools.combinations(goodwords, 2) # set this to number of words\n",
        "        for acombo in allcombos:\n",
        "          row = []\n",
        "          for anode in acombo:\n",
        "            row.append(anode)\n",
        "          row.append('Undirected')\n",
        "          csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuhw_i0csmZR",
        "colab_type": "text"
      },
      "source": [
        "# Final Semantic Edgelist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSgZa0qgaQOT",
        "colab_type": "code",
        "outputId": "cd861d5a-7d42-4807-b50c-aa1530901a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    userwhotweeted = tweetjson['user']['screen_name']\n",
        "    if userwhotweeted not in botdata:\n",
        "      #natural language preprocessing- cleaning the tweets\n",
        "      nourlstext = removeURL(text)\n",
        "      tokenizedtext = tokenize(nourlstext)\n",
        "      nostopwordstext = stopWords(tokenizedtext)\n",
        "      lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "      nopuncttext = removePunctuation(lemmatizedtext)\n",
        "      #sleep(10)\n",
        "\n",
        "      #print(tokenizedtext)\n",
        "      #print(nostopwordstext)\n",
        "      #print(lemmatizedtext)\n",
        "      #print(nopuncttext)\n",
        "\n",
        "      for aword in nopuncttext:\n",
        "        if aword in uniquewords:\n",
        "          uniquewords[aword] += 1\n",
        "        if aword not in uniquewords:\n",
        "          uniquewords[aword] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4_sbiwoahtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of unique words: key - word; value = number of times the word appears\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "  if uniquewords[aword] > 25: #change number to get around 1000 words\n",
        "    wordcount += 1\n",
        "    wordstoinclude.add(aword)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM1iTdvTanKJ",
        "colab_type": "code",
        "outputId": "67c2b466-e3b6-4350-f29e-dc0368c1281e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "edgelist = open('/content/drive/My Drive/Colab Notebooks/Network Analysis/final.sentiment.artists.semantic.edgelistforgephi.csv', 'w') \n",
        "csvwriter = csv.writer(edgelist)\n",
        "header = ['Source', 'Target', 'Type'] # add Type to mention this is undirected \n",
        "csvwriter.writerow(header)\n",
        "\n",
        "print('Writing Edge List')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Edge List\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUvD5hL6ap3l",
        "colab_type": "code",
        "outputId": "d03e9a99-625f-45c7-ed98-8bd1ac531b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    textannotation = TextBlob(text)\n",
        "    sentimentscore = textannotation.sentiment.polarity\n",
        "    if userwhotweeted not in botdata:\n",
        "    #natural language preprocessing- cleaning the tweets\n",
        "      nourlstext = removeURL(text)\n",
        "      tokenizedtext = tokenize(nourlstext)\n",
        "      nostopwordstext = stopWords(tokenizedtext)\n",
        "      lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "      nopuncttext = removePunctuation(lemmatizedtext)\n",
        "\n",
        "      goodwords = []\n",
        "      for aword in nopuncttext:\n",
        "        if aword in wordstoinclude:\n",
        "          goodwords.append(aword.replace(',',''))\n",
        "      \n",
        "      allcombos = itertools.combinations(goodwords, 2) # set this to number of words\n",
        "      for acombo in allcombos:\n",
        "        row = []\n",
        "        for anode in acombo:\n",
        "          row.append(anode)\n",
        "        row.append('Undirected')\n",
        "        csvwriter.writerow(row)\n",
        "edgelist.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJz2hymFr70Z",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Mention Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBqPH5MzvU4O",
        "colab_type": "text"
      },
      "source": [
        "All Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1N8Fv4rPYJBa1P8vTwFib8DsNzfNF-Pc8)\n",
        "\n",
        "High Follower Count Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1Egr2N9_JxVUFLmnRadwKaLO-IzmRvlOz)\n",
        "\n",
        "Low Follower Count Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=13EaYiPknud7GLDlZcrJtPJZLdCoJ8Z05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tncqnuE2r0m1",
        "colab_type": "text"
      },
      "source": [
        "# Who are the Most Central Users?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLF-LIEos9-Q",
        "colab_type": "text"
      },
      "source": [
        "Taylor Swift's Most Central Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1tlunnWWK7KlWSBYTB2Q5YUwie07JB8dm)\n",
        "\n",
        "*   @noahlevy13\n",
        "*   @VodGrey\n",
        "*   @luizaswift13\n",
        "*   @taylornation13\n",
        "*   @tragicredlovers\n",
        "*   @ContaDe29876869\n",
        "*   @LightCamiller\n",
        "*   @Taylorvotesbr\n",
        "*   @starlight13T\n",
        "*   @viihzen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRZ7hy2vPu9",
        "colab_type": "text"
      },
      "source": [
        "Katy Perry's Most Central Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=15ksBioe30pDuGcnCT9Rdlpubnv0ltNBg)\n",
        "![alt text](https://drive.google.com/uc?id=15ISOPPJ1NLnq2JqR6rjwFZ_aPaR9K57Y)\n",
        "\n",
        "*   @KatyPerryFanss\n",
        "*   @ABCNetwork\n",
        "*   @claudiaa_jordan\n",
        "*   @AndTheWriterIs\n",
        "*   @katys_old\n",
        "*   @kathery51171549\n",
        "*   @AroonDani1\n",
        "*   @tonooneelse\n",
        "*   @MikeleKatyCat\n",
        "*   @iamvikashkumr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idXbdZIpvP5X",
        "colab_type": "text"
      },
      "source": [
        "Lady Gaga's Most Central Users:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1nTCh-o6e7bGemWdS2v09gEizGZnpHCOO)\n",
        "![alt text](https://drive.google.com/uc?id=18b3uudKsPDQ1HAodRAbst-62mabcarFX)\n",
        "\n",
        "*   @harrysenigma\n",
        "*   @carlosgzz03\n",
        "*   @hauslabs\n",
        "*   @facedbykareem\n",
        "*   @stan_LadyGaga\n",
        "*   @hauntedIllusion\n",
        "*   @Harry_Styles\n",
        "*   @LadyGaga_Taiwan\n",
        "*   @monstershallow\n",
        "*   @IvyPark_outsold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO7wJz_ynOjb",
        "colab_type": "text"
      },
      "source": [
        "These central users are really engaged with Taylor Swift, Katy Perry, and Lady Gaga because they are fans, want to talk about them, and maybe even get a response from their idols. These fans should be in the artist that they support's street team because they are passionate and will spread news about the artist quickly. \n",
        "\n",
        "ABC Network is a central user for Katy Perry because it frequently talks about Katy Perry as one of American Idol's judges. \n",
        "\n",
        "Since Harry Styles is a central user for Lady Gaga and is famous himself, Lady Gaga should definitely ask him to post whenever she has new content about her upcoming album."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9639jT3CtDDq",
        "colab_type": "text"
      },
      "source": [
        "# Who are the Most Important Bridgers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLpKZ-3azsUx",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1_uAIBaCIPqHIY3Pl5jxg7dy4jGL72UwY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8OodcObtFA-",
        "colab_type": "text"
      },
      "source": [
        " @AndTheWriterIs is an important bridge because the podcast discusses songwriting and Lady Gaga, Katy Perry, and Taylor's Swift's songs. The show will be interested whenever they release a new song because they will want to discuss it.\n",
        "\n",
        "@BillieEilish is a fan of Taylor, Katy, and Lady Gaga, so reaching out to her for collaborations or promotions would be helpful as she is famous herself.\n",
        "\n",
        "@billboard is an important bridge as it is an important music magazine that tweets about all of them.\n",
        "\n",
        "@TSwiftNZ and @IvyPark_outsold are big fans of Taylor Swift, Lady Gaga, and Katy Perry, so they would be interested in new pop stars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYtpHdSsBL3",
        "colab_type": "text"
      },
      "source": [
        "# Semantic Network Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeHt5_eUlqQg",
        "colab_type": "text"
      },
      "source": [
        "All Sentiment: ![alt text](https://drive.google.com/uc?id=1C6G04GBWa-BWe2xCRJ3mpvw02z3Xf-UW)\n",
        "Negative Sentiment: ![alt text](https://drive.google.com/uc?id=188NZr3jCG7b9Vd037F_rplQXlSFdnpIx)\n",
        "Positive Sentiment: ![alt text](https://drive.google.com/uc?id=1ZR9wgjHNbMuAnAnxYhA2eTGGCIhanjp3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEF3fRUxtF0s",
        "colab_type": "text"
      },
      "source": [
        "# Semantic Word Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKDnVldLspSK",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1UiG6THPbUpJydB297YQbTpC8uV1wB9lw)\n",
        "![alt text](https://drive.google.com/uc?id=1wVcBDyizFHcg_HDHeVu-qhNU1DOrllvy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cl4PZXWtKi5",
        "colab_type": "text"
      },
      "source": [
        "Katy Perry fans are mad and complaining that it's taking a long time for her to release a new album.\n",
        "\n",
        "Katy Perry fans are also complaining that her Shoesday Tuesday discounts on her kpcollections shoes are \"pitiful.\"\n",
        "\n",
        "People are talking about Katy Perry on American Idol.\n",
        "\n",
        "People hate Taylor Swift's new single.\n",
        "\n",
        "Fans are talking about Katy Perry's new partnership with Amazon Music.\n",
        "\n",
        "Lady Gaga owns the word monster from her little monster fans.\n",
        "\n",
        "Katy Perry tweets a lot about her labelmate Cynthia Lovely.\n",
        "\n",
        "People were comparing Lady Gaga's Super Bowl performance to Jennifer Lopez's this year.\n",
        "\n",
        "A lot of people are talking about wanting to preorder Lady Gaga and Katy Perry's new albums on colectible vinyl.\n",
        "\n",
        "People are really amped about Lady Gaga's new amazing single \"Stupid Love.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBniAyGMtLSN",
        "colab_type": "text"
      },
      "source": [
        "# What Word Clusters Does Each Brand Own?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pNsZxoxtOL-",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1LTuxL-Z8AJuaOjPXeXWLgW8Vtcmm18Vq)\n",
        "![alt text](https://drive.google.com/uc?id=1J28pd_-JTdQlpkk-Tg4ue01ttHIKP2WV)\n",
        "![alt text](https://drive.google.com/uc?id=1gkOQbwF0IsIyZotnIHdpAFXQnVjsa7AS)\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1hvwDJmbkMTGx4_f3VZuriYd3BDI1vgi4)\n",
        "\n",
        "Lady Gaga owns monsters, hauslabs, amp, lg6, love, and thank. \n",
        "Thank and love are because fans are thanking her for releasing \"Stupid Love.\"\n",
        "\n",
        "Taylor Swift owns 1 and 3 because her favorite number is 13. She also owns favorite because of this number and because she is fans' favorite artist. She owns taylornation13 because that is her fan club.\n",
        "\n",
        "Katy Perry owns best and American Idol because people think she is the best judge on the show. Katy Perry also owns shoes because she is the only one with a shoe line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgXQckAHtXQx",
        "colab_type": "text"
      },
      "source": [
        "#Bridging Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkuTj_J2tQjj",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1H1jBCTmig8qR-gknqeb75EMq3bmxeEzu)\n",
        "\n",
        "Although Lady Gaga has a stronger hold on love right now, she also bridges love and lover with Taylor Swift because Taylor's latest album is Lover and most of her songs are about love.\n",
        "\n",
        "Katy Perry, Lady Gaga, and Taylor Swift all share queen, music, album, song, and video because they are queens of pop music.\n"
      ]
    }
  ]
}